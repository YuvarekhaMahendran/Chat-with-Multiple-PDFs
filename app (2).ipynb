{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "VajRV9NqO1P0",
      "metadata": {
        "id": "VajRV9NqO1P0"
      },
      "source": [
        "**Project Title: Chat with Multiple PDFs**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ExdQSlSVDnpv",
      "metadata": {
        "id": "ExdQSlSVDnpv"
      },
      "source": [
        "### Team Members"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sghZPYhKD6_q",
      "metadata": {
        "id": "sghZPYhKD6_q"
      },
      "source": [
        "Abhishek Basnet\n",
        "\n",
        "Sushmitha Ugle Ashok\n",
        "\n",
        "Yuvarekha Mahendran"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CNWbzmB3gMZu",
      "metadata": {
        "id": "CNWbzmB3gMZu"
      },
      "source": [
        "### Required libraries to be installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qyaKvTFxgL-p",
      "metadata": {
        "id": "qyaKvTFxgL-p"
      },
      "outputs": [],
      "source": [
        "pip install -q streamlit dotenv PyPDF2 langchain FAISS openai tiktoken\n",
        "# streamlit: for building the web interface\n",
        "# dotenv: for managing environment variables (e.g., API keys)\n",
        "# PyPDF2: for extracting text from PDF files\n",
        "# langchain: for text processing, embeddings, and conversational AI\n",
        "# FAISS: for efficient similarity search in vector databases\n",
        "# openai: for integrating OpenAI language models\n",
        "# tiktoken: for tokenizing text for OpenAI models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda4addf",
      "metadata": {
        "id": "fda4addf",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import streamlit as st  # For creating the web application\n",
        "from dotenv import load_dotenv  # To load environment variables from a .env file\n",
        "from PyPDF2 import PdfReader  # For reading PDF files\n",
        "from langchain.text_splitter import CharacterTextSplitter  # For splitting text into manageable chunks\n",
        "from langchain.embeddings import OpenAIEmbeddings  # To generate text embeddings\n",
        "from langchain.vectorstores import FAISS  # For storing and querying vectors efficiently\n",
        "from langchain.chat_models import ChatOpenAI  # Chat model interface for OpenAI GPT\n",
        "from langchain.memory import ConversationBufferMemory  # To store conversation history in memory\n",
        "from langchain.chains import ConversationalRetrievalChain  # Conversational chain that retrieves relevant data\n",
        "from htmlTemplates import css, bot_template, user_template  # Custom HTML templates for styling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9EcFEiEVjCkY",
      "metadata": {
        "id": "9EcFEiEVjCkY"
      },
      "source": [
        "\n",
        "### Function to extract text from PDF documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c39e31b7",
      "metadata": {
        "id": "c39e31b7"
      },
      "outputs": [],
      "source": [
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"  # Initialize an empty string to hold the extracted text\n",
        "    for pdf in pdf_docs:  # Iterate through each uploaded PDF\n",
        "        pdf_reader = PdfReader(pdf)  # Create a PDF reader object\n",
        "        for page in pdf_reader.pages:  # Iterate through all pages in the PDF\n",
        "            text += page.extract_text()  # Extract and append text from each page\n",
        "    return text  # Return the combined text\n",
        "  # Extracts text from a list of uploaded PDF files and Combined text extracted from all the pages of all PDFs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rfECbzflkpNg",
      "metadata": {
        "id": "rfECbzflkpNg"
      },
      "source": [
        "### Function to split text into smaller chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86849958",
      "metadata": {
        "id": "86849958"
      },
      "outputs": [],
      "source": [
        "def get_text_chunks(text):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",  # Split text by new lines\n",
        "        chunk_size=1000,  # Maximum size of each text chunk\n",
        "        chunk_overlap=200,  # Overlap size between consecutive chunks for context\n",
        "        length_function=len  # Function to determine the length of the text\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)  # Perform the text splitting\n",
        "    return chunks  # Return the list of text chunks\n",
        " # Splits a large text into smaller chunks for easier processing\n",
        " # Looks for the newline separator (\\n) to split the text. If no newlines are found, it will fall back to splitting based on the chunk_size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DF0f1yYinHq8",
      "metadata": {
        "id": "DF0f1yYinHq8"
      },
      "source": [
        "Function to create a vectorstore using FAISS for storing embeddings of text chunks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3207a69d",
      "metadata": {
        "id": "3207a69d"
      },
      "outputs": [],
      "source": [
        "def get_vectorstore(text_chunks):\n",
        "    embeddings = OpenAIEmbeddings() # Initializing the embedding model from OpenAI to generate text embeddings to transform chunks to embeddings\n",
        "    #numeric representations of words, phrases, or entire texts that capture meaning(multi-dimension)\n",
        "\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    #vectors-represents data in numeric form(specific dimension)\n",
        "    #FAISS (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of documents that are similar to each other.\n",
        "\n",
        "    return vectorstore # Returns the created FAISS vectorstore containing the text embeddings(storage mechanism for embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20SAi_SNn44_",
      "metadata": {
        "id": "20SAi_SNn44_"
      },
      "source": [
        "Function to create a conversational chain for handling chat interactions using LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12430f8d",
      "metadata": {
        "id": "12430f8d"
      },
      "outputs": [],
      "source": [
        " def get_conversation_chain(vectorstore): #To set up a conversational interface that uses the data.\n",
        "    llm = ChatOpenAI(openai_api_key=\"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\")\n",
        "    #An LLM is a predictive engine that generates text with past data.\n",
        "    #OpenAI API key is a unique identifier provided by OpenAI to authenticate users to access their API.\n",
        "\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True)\n",
        "    # Setting up memory to track the chat history during the conversation\n",
        "\n",
        "    # 'return_messages=True' ensures that previous messages are accessible for context\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    #integrates natural language with external knowledge base like vectorstore.\n",
        "    #`retriever=vectorstore.as_retriever()`: Configures the vectorstore to retrieve relevant data during the conversation\n",
        "\n",
        "    return conversation_chain #Returns the configured conversation chain for use in handling conversational queries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bHlhkksoaNf",
      "metadata": {
        "id": "6bHlhkksoaNf"
      },
      "source": [
        "Function to process and handle the user's input question or query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m-MJCgVsbkM2",
      "metadata": {
        "id": "m-MJCgVsbkM2"
      },
      "outputs": [],
      "source": [
        "def handle_userinput(user_question):\n",
        "    # Using the user question to get a response from the conversation chain.\n",
        "    # 'conversation' is a session variable that stores the AI chat model.\n",
        "    # The function call retrieves a response based on the current user question.\n",
        "    response = st.session_state.conversation({'question': user_question})\n",
        "\n",
        "    # Store the chat history returned from the response in the session state(feature to store variables across different runtimes).\n",
        "    # 'chat_history' is updated with the complete conversation history, including both user and AI messages.\n",
        "    st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "    # Iterating through each message in the chat history. Each message has two parts: user input and bot response.\n",
        "    # The loop uses 'enumerate' to keep track of the index (i) of each message in the chat history.\n",
        "    for i, message in enumerate(st.session_state.chat_history):\n",
        "        # Check if the index (i) is even. If it's even, it's a user message.The chat history is assumed to store messages in an alternating pattern: user message first, then bot response\n",
        "        if i % 2 == 0:\n",
        "            # Replacing the {{MSG}} placeholder in the user_template with the user's message content.\n",
        "            # Then display the formatted HTML using Streamlit's st.write method.\n",
        "            st.write(user_template.replace(\n",
        "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "        else:\n",
        "            # If the index (i) is odd, it's a bot (AI) response.\n",
        "            # Replacing the {{MSG}} placeholder in the bot_template with the bot's message content.\n",
        "            # Then displaying the formatted HTML using Streamlit's st.write method.\n",
        "            st.write(bot_template.replace(\n",
        "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c462ff",
      "metadata": {
        "id": "e3c462ff"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    load_dotenv()\n",
        "    st.set_page_config(page_title=\"Chat with multiple PDFs\",\n",
        "                       page_icon=\"📝\")\n",
        "    st.write(css, unsafe_allow_html=True) #using css template to write\n",
        "\n",
        "    if \"conversation\" not in st.session_state:\n",
        "        st.session_state.conversation = None\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = None\n",
        "\n",
        "    st.header(\"Chat with multiple PDFs📝\")\n",
        "    user_question = st.text_input(\"Ask a question about your documents:\")\n",
        "    if user_question:\n",
        "        handle_userinput(user_question)\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.subheader(\"Your documents\")\n",
        "        pdf_docs = st.file_uploader(\n",
        "            \"Upload your PDFs here and click on 'Process'\", accept_multiple_files=True)\n",
        "        if st.button(\"Process\"):\n",
        "            with st.spinner(\"Processing\"):\n",
        "                # get pdf text\n",
        "                raw_text = get_pdf_text(pdf_docs)\n",
        "\n",
        "                # get the text chunks\n",
        "                text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "                # create vector store\n",
        "                vectorstore = get_vectorstore(text_chunks)\n",
        "\n",
        "                # create conversation chain\n",
        "                st.session_state.conversation = get_conversation_chain(\n",
        "                    vectorstore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e12afe05",
      "metadata": {
        "id": "e12afe05"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab5b28a",
      "metadata": {
        "id": "3ab5b28a"
      },
      "source": [
        "Chatbot With One PDF![Chat%20with%20multiple%20PDFs.png](attachment:Chat%20with%20multiple%20PDFs.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1241e0d",
      "metadata": {
        "id": "a1241e0d"
      },
      "source": [
        "Chatbot With Mulitple PDF's\n",
        "![Chat%20with%20multiple%20PDFs.png](attachment:Chat%20with%20multiple%20PDFs.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44717d29",
      "metadata": {
        "id": "44717d29"
      },
      "source": [
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b2c2e39",
      "metadata": {
        "id": "1b2c2e39"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IJVy4Ein7mCp",
      "metadata": {
        "id": "IJVy4Ein7mCp"
      },
      "source": [
        "**requirements.txt File**\n",
        "\n",
        "\n",
        "langchain==0.0.184\n",
        "\n",
        "PyPDF2==3.0.1\n",
        "\n",
        "python-dotenv==1.0.0\n",
        "\n",
        "streamlit==1.18.1\n",
        "\n",
        "openai==0.27.6\n",
        "\n",
        "faiss-cpu==1.7.4\n",
        "\n",
        "altair==4\n",
        "\n",
        "tiktoken==0.4.0"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
